{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 21:47:12.502409: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-10 21:47:12.519241: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731255432.542218   13052 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731255432.549057   13052 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-10 21:47:12.571810: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from transformers import LEDTokenizer, LEDForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ECTSum' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/rajdeep345/ECTSum.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ECTS files loaded: 1681\n",
      "Number of Summaries files loaded: 1681\n"
     ]
    }
   ],
   "source": [
    "ects_path = \"ECTSum/data/final/train/ects/\"\n",
    "summaries_path = \"ECTSum/data/final/train/gt_summaries/\"\n",
    "\n",
    "# Read ects text files\n",
    "ects_files = glob.glob(os.path.join(ects_path, \"*.txt\"))\n",
    "ects_data = {}\n",
    "for filepath in ects_files:\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        filename = os.path.basename(filepath)\n",
    "        ects_data[filename] = file.read()\n",
    "\n",
    "# Read gt_summaries text files\n",
    "summaries_files = glob.glob(os.path.join(summaries_path, \"*.txt\"))\n",
    "summaries_data = {}\n",
    "for filepath in summaries_files:\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        filename = os.path.basename(filepath)\n",
    "        summaries_data[filename] = file.read()\n",
    "\n",
    "# ects_data and summaries_data now contain the contents of the files\n",
    "print(\"Number of ECTS files loaded:\", len(ects_data))\n",
    "print(\"Number of Summaries files loaded:\", len(summaries_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ECTS files loaded: 249\n",
      "Number of Summaries files loaded: 249\n"
     ]
    }
   ],
   "source": [
    "val_ects_path = \"ECTSum/data/final/val/ects/\"\n",
    "val_summaries_path = \"ECTSum/data/final/val/gt_summaries/\"\n",
    "\n",
    "# Read ects text files\n",
    "val_ects_files = glob.glob(os.path.join(val_ects_path, \"*.txt\"))\n",
    "val_ects_data = {}\n",
    "for filepath in val_ects_files:\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        filename = os.path.basename(filepath)\n",
    "        val_ects_data[filename] = file.read()\n",
    "\n",
    "# Read gt_summaries text files\n",
    "val_summaries_files = glob.glob(os.path.join(val_summaries_path, \"*.txt\"))\n",
    "val_summaries_data = {}\n",
    "for filepath in val_summaries_files:\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        filename = os.path.basename(filepath)\n",
    "        val_summaries_data[filename] = file.read()\n",
    "\n",
    "# ects_data and summaries_data now contain the contents of the files\n",
    "print(\"Number of ECTS files loaded:\", len(val_ects_data))\n",
    "print(\"Number of Summaries files loaded:\", len(val_summaries_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ECTS files loaded: 495\n",
      "Number of Summaries files loaded: 495\n"
     ]
    }
   ],
   "source": [
    "test_ects_path = \"ECTSum/data/final/test/ects/\"\n",
    "test_summaries_path = \"ECTSum/data/final/test/gt_summaries/\"\n",
    "\n",
    "# Read ects text files\n",
    "test_ects_files = glob.glob(os.path.join(test_ects_path, \"*.txt\"))\n",
    "test_ects_data = {}\n",
    "for filepath in test_ects_files:\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        filename = os.path.basename(filepath)\n",
    "        test_ects_data[filename] = file.read()\n",
    "\n",
    "# Read gt_summaries text files\n",
    "test_summaries_files = glob.glob(os.path.join(test_summaries_path, \"*.txt\"))\n",
    "test_summaries_data = {}\n",
    "for filepath in test_summaries_files:\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        filename = os.path.basename(filepath)\n",
    "        test_summaries_data[filename] = file.read()\n",
    "\n",
    "# test_ects_data and test_summaries_data now contain the contents of the files\n",
    "print(\"Number of ECTS files loaded:\", len(test_ects_data))\n",
    "print(\"Number of Summaries files loaded:\", len(test_summaries_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Before we begin our call today, I want to remi...</td>\n",
       "      <td>compname posts q2 adjusted loss per share $0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm Kelsey Duffy, Vice President of Investor R...</td>\n",
       "      <td>compname reports q3 earnings per share $1.66.\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We have slides for our conference call.\\nYou c...</td>\n",
       "      <td>compname says recent acquisitions to add fy sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sales in the second quarter were $129.6 millio...</td>\n",
       "      <td>q2 earnings per share $0.03.\\nq2 sales $129.6 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We begin with our Safe Harbor statement.\\nYou ...</td>\n",
       "      <td>sees fy adjusted earnings per share $14.75 to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1676</th>\n",
       "      <td>Slide two contains our Safe Harbor statement.\\...</td>\n",
       "      <td>compname reports q3 adjusted earnings per shar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>We appreciate you're joining us today for Gart...</td>\n",
       "      <td>q1 revenue rose 8.4 percent to $1.1 billion.\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1678</th>\n",
       "      <td>With us on the call today are Michael Kasbar, ...</td>\n",
       "      <td>compname reports q1 adjusted earnings per shar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679</th>\n",
       "      <td>Yesterday after the close of market, we announ...</td>\n",
       "      <td>compname reports q1 earnings per share $0.17.\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>I'm pleased that you're able to join us today....</td>\n",
       "      <td>compname reports q1 earnings per share $1.07.\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1681 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     Before we begin our call today, I want to remi...   \n",
       "1     I'm Kelsey Duffy, Vice President of Investor R...   \n",
       "2     We have slides for our conference call.\\nYou c...   \n",
       "3     Sales in the second quarter were $129.6 millio...   \n",
       "4     We begin with our Safe Harbor statement.\\nYou ...   \n",
       "...                                                 ...   \n",
       "1676  Slide two contains our Safe Harbor statement.\\...   \n",
       "1677  We appreciate you're joining us today for Gart...   \n",
       "1678  With us on the call today are Michael Kasbar, ...   \n",
       "1679  Yesterday after the close of market, we announ...   \n",
       "1680  I'm pleased that you're able to join us today....   \n",
       "\n",
       "                                                summary  \n",
       "0     compname posts q2 adjusted loss per share $0.1...  \n",
       "1     compname reports q3 earnings per share $1.66.\\...  \n",
       "2     compname says recent acquisitions to add fy sa...  \n",
       "3     q2 earnings per share $0.03.\\nq2 sales $129.6 ...  \n",
       "4     sees fy adjusted earnings per share $14.75 to ...  \n",
       "...                                                 ...  \n",
       "1676  compname reports q3 adjusted earnings per shar...  \n",
       "1677  q1 revenue rose 8.4 percent to $1.1 billion.\\n...  \n",
       "1678  compname reports q1 adjusted earnings per shar...  \n",
       "1679  compname reports q1 earnings per share $0.17.\\...  \n",
       "1680  compname reports q1 earnings per share $1.07.\\...  \n",
       "\n",
       "[1681 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names = []\n",
    "for file_name in ects_data.keys():\n",
    "    file_names.append(file_name)\n",
    "data = []\n",
    "for file_name in file_names:\n",
    "    data_dict = {'text':ects_data[file_name], 'summary': summaries_data[file_name]}\n",
    "    data.append(data_dict)\n",
    "train_df = pd.DataFrame(data)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well, we had an exciting quarter.\\nI'm not sur...</td>\n",
       "      <td>compname reports q3 ffo per share $1.85 exclud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Please note that this conference is being reco...</td>\n",
       "      <td>mdc holdings sees q4 2020 home deliveries betw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Before we get started, I want to let you know ...</td>\n",
       "      <td>q1 gaap earnings per share $5.52.\\nsees fy adj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carter's is making good progress, recovering f...</td>\n",
       "      <td>compname reports q3 earnings per share of $1.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dollargeneral.com under News &amp; Events.\\nWe als...</td>\n",
       "      <td>q4 earnings per share $2.62.\\nq4 same store sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>Factors, to our Annual Report on Form 10-K for...</td>\n",
       "      <td>abbott sold more than 225 mln covid-19 tests g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>If you do not yet have a copy, you can access ...</td>\n",
       "      <td>q4 adjusted ffo loss per share $0.16.17 hotel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Kessel Stelling, chairman and chief executive ...</td>\n",
       "      <td>compname announces q3 earnings per share $0.56...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>In particular, the extent of the continued imp...</td>\n",
       "      <td>q2 non-gaap earnings per share $0.91.\\nq2 gaap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>We begin with our Safe Harbor statement.\\nYou ...</td>\n",
       "      <td>sees fy adjusted earnings per share $15.00 to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>249 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    Well, we had an exciting quarter.\\nI'm not sur...   \n",
       "1    Please note that this conference is being reco...   \n",
       "2    Before we get started, I want to let you know ...   \n",
       "3    Carter's is making good progress, recovering f...   \n",
       "4    dollargeneral.com under News & Events.\\nWe als...   \n",
       "..                                                 ...   \n",
       "244  Factors, to our Annual Report on Form 10-K for...   \n",
       "245  If you do not yet have a copy, you can access ...   \n",
       "246  Kessel Stelling, chairman and chief executive ...   \n",
       "247  In particular, the extent of the continued imp...   \n",
       "248  We begin with our Safe Harbor statement.\\nYou ...   \n",
       "\n",
       "                                               summary  \n",
       "0    compname reports q3 ffo per share $1.85 exclud...  \n",
       "1    mdc holdings sees q4 2020 home deliveries betw...  \n",
       "2    q1 gaap earnings per share $5.52.\\nsees fy adj...  \n",
       "3    compname reports q3 earnings per share of $1.8...  \n",
       "4    q4 earnings per share $2.62.\\nq4 same store sa...  \n",
       "..                                                 ...  \n",
       "244  abbott sold more than 225 mln covid-19 tests g...  \n",
       "245  q4 adjusted ffo loss per share $0.16.17 hotel ...  \n",
       "246  compname announces q3 earnings per share $0.56...  \n",
       "247  q2 non-gaap earnings per share $0.91.\\nq2 gaap...  \n",
       "248  sees fy adjusted earnings per share $15.00 to ...  \n",
       "\n",
       "[249 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names_val = []\n",
    "for file_name in val_ects_data.keys():\n",
    "    file_names_val.append(file_name)\n",
    "val_data = []\n",
    "for file_name in file_names_val:\n",
    "    val_data_dict = {'text':val_ects_data[file_name], 'summary': val_summaries_data[file_name]}\n",
    "    val_data.append(val_data_dict)\n",
    "val_df = pd.DataFrame(val_data)\n",
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They are based on management's assumptions, wh...</td>\n",
       "      <td>compname reports q2 loss per share $0.17 from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the third quarter Cullen/Frost earned $109....</td>\n",
       "      <td>q3 earnings per share $1.73.\\nincreases quarte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>With me on the call today are Seamus Grady, Ch...</td>\n",
       "      <td>compname reports q2 revenue $453.8 mln.\\nsees ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>These factors are detailed in the company's fi...</td>\n",
       "      <td>qtrly optical communications sales grew 24% yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020 was an extraordinary year, and the consis...</td>\n",
       "      <td>compname reports earnings per share of $2.54, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>Today, we will review the second quarter 2021 ...</td>\n",
       "      <td>q2 non-gaap earnings per share $3.54.\\nq2 earn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>Jim Hatfield, Chief Administrative Officer; Da...</td>\n",
       "      <td>compname reports q1 earnings per share of $0.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>I will then review our segment performance and...</td>\n",
       "      <td>compname reports qtrly adjusted earnings per s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>These statements reflect the participants' exp...</td>\n",
       "      <td>q1 non-gaap earnings per share $0.79 from cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>One quick bit of housekeeping.\\nCory and I wil...</td>\n",
       "      <td>q3 adjusted non-gaap earnings per share $3.98....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>495 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    They are based on management's assumptions, wh...   \n",
       "1    In the third quarter Cullen/Frost earned $109....   \n",
       "2    With me on the call today are Seamus Grady, Ch...   \n",
       "3    These factors are detailed in the company's fi...   \n",
       "4    2020 was an extraordinary year, and the consis...   \n",
       "..                                                 ...   \n",
       "490  Today, we will review the second quarter 2021 ...   \n",
       "491  Jim Hatfield, Chief Administrative Officer; Da...   \n",
       "492  I will then review our segment performance and...   \n",
       "493  These statements reflect the participants' exp...   \n",
       "494  One quick bit of housekeeping.\\nCory and I wil...   \n",
       "\n",
       "                                               summary  \n",
       "0    compname reports q2 loss per share $0.17 from ...  \n",
       "1    q3 earnings per share $1.73.\\nincreases quarte...  \n",
       "2    compname reports q2 revenue $453.8 mln.\\nsees ...  \n",
       "3    qtrly optical communications sales grew 24% yo...  \n",
       "4    compname reports earnings per share of $2.54, ...  \n",
       "..                                                 ...  \n",
       "490  q2 non-gaap earnings per share $3.54.\\nq2 earn...  \n",
       "491  compname reports q1 earnings per share of $0.3...  \n",
       "492  compname reports qtrly adjusted earnings per s...  \n",
       "493  q1 non-gaap earnings per share $0.79 from cont...  \n",
       "494  q3 adjusted non-gaap earnings per share $3.98....  \n",
       "\n",
       "[495 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names_test = []\n",
    "for file_name in test_ects_data.keys():\n",
    "    file_names_test.append(file_name)\n",
    "\n",
    "test_data = []\n",
    "for file_name in file_names_test:\n",
    "    test_data_dict = {'text': test_ects_data[file_name], 'summary': test_summaries_data[file_name]}\n",
    "    test_data.append(test_data_dict)\n",
    "\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Initialize the T5 tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# Define max lengths\n",
    "max_input_length = 4096\n",
    "max_output_length = 256\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(row):\n",
    "    text = row['text']\n",
    "    summary = row['summary']\n",
    "    \n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(\n",
    "        f\"summarize: {text}\",\n",
    "        max_length=max_input_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Tokenize target summary\n",
    "    targets = tokenizer(\n",
    "        summary,\n",
    "        max_length=max_output_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "        \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "        \"labels\": targets[\"input_ids\"].squeeze()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the entire dataset\n",
    "train_data = [preprocess_data(row) for _, row in train_df.iterrows()]\n",
    "val_data = [preprocess_data(row) for _, row in val_df.iterrows()]\n",
    "test_data = [preprocess_data(row) for _, row in test_df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'summary'],\n",
       "    num_rows: 1681\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amitesh_sah/anaconda3/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, AdamW\n",
    "\n",
    "# Initialize the T5 model and move it to the GPU if available\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                                         | 0/1681 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed, Average Loss: 1.0899445229253728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed, Average Loss: 0.658503212622527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed, Average Loss: 0.5881197213276707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 completed, Average Loss: 0.5466246971697644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 completed, Average Loss: 0.5184591745077934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Training loop with progress bar\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0  # To accumulate the loss for averaging\n",
    "    \n",
    "    # Wrap the data iterator with tqdm to display a progress bar\n",
    "    progress_bar = tqdm(train_data, desc=f\"Epoch {epoch + 1}\", leave=False)\n",
    "    step = 0\n",
    "    \n",
    "    for data in progress_bar:\n",
    "        input_ids = data[\"input_ids\"].unsqueeze(0).to(device)\n",
    "        attention_mask = data[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "        labels = data[\"labels\"].unsqueeze(0).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()  # Accumulate the loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        # Update progress bar with the current loss\n",
    "        progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    avg_loss = total_loss / step\n",
    "    print(f\"Epoch {epoch + 1} completed, Average Loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2abc43b3d91142e1b4e479a5303ae4a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b50f5b0b434957aafefc576ce9eca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770aa3191b494b608955707b070ed801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab978d225a7343eabb27439600ed70c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/amitesh_sah/nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/amitesh_sah/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/amitesh_sah/nltk_data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eafdde67becc496da5fb7017e4bba931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load all necessary metrics\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "bert_score_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "model.to('cuda')\n",
    "\n",
    "# Evaluation function to compute ROUGE, BLEU, BERTScore, and METEOR on test data\n",
    "def evaluate_model_on_test(test_dataset):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for batch in tqdm(test_dataset, desc=\"Evaluating\", leave=False):\n",
    "            # Convert input_ids and attention_mask to tensors\n",
    "            input_ids = torch.tensor(batch[\"input_ids\"]).unsqueeze(0).to('cuda')  # Add batch dimension\n",
    "            attention_mask = torch.tensor(batch[\"attention_mask\"]).unsqueeze(0).to('cuda')\n",
    "            \n",
    "            # Generate summary\n",
    "            generated_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=150)\n",
    "            pred_summary = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            true_summary = tokenizer.decode(batch[\"labels\"], skip_special_tokens=True)\n",
    "            \n",
    "            # Collect predictions and references\n",
    "            predictions.append(pred_summary)\n",
    "            references.append(true_summary)\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    rouge_scores = rouge_metric.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "    \n",
    "    # Compute BLEU scores (requires references as a list of lists)\n",
    "    bleu_scores = bleu_metric.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    \n",
    "    # Compute METEOR scores\n",
    "    meteor_scores = meteor_metric.compute(predictions=predictions, references=references)\n",
    "    \n",
    "    # Compute BERTScore (useful for semantic similarity)\n",
    "    bert_scores = bert_score_metric.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "    bert_f1 = sum(bert_scores[\"f1\"]) / len(bert_scores[\"f1\"])  # Average F1 score from BERTScore\n",
    "    \n",
    "    # Return all metrics\n",
    "    return {\n",
    "        \"rouge1\": rouge_scores[\"rouge1\"],\n",
    "        \"rouge2\": rouge_scores[\"rouge2\"],\n",
    "        \"rougeL\": rouge_scores[\"rougeL\"],\n",
    "        \"bleu\": bleu_scores[\"bleu\"],\n",
    "        \"meteor\": meteor_scores[\"meteor\"],\n",
    "        \"bertscore_f1\": bert_f1  # BERTScore F1 (semantic similarity score)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|                                       | 0/495 [00:00<?, ?it/s]/tmp/ipykernel_13052/3850840905.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(batch[\"input_ids\"]).unsqueeze(0).to('cuda')  # Add batch dimension\n",
      "/tmp/ipykernel_13052/3850840905.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.tensor(batch[\"attention_mask\"]).unsqueeze(0).to('cuda')\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3b3092c78046cfb2bdb1ae4f4a6ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ba54bfb4154ffab38dc52adc1285a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025b716204844508bcbfe7a6ddace98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b29f7bed01483ba61c05f6f738009d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034d9ca799a140ad96891b51fc44722d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f65e687e994f59a6793ed635c004a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results on Test dataset:\n",
      "{'rouge1': 0.3562540031356058, 'rouge2': 0.23702649554799832, 'rougeL': 0.326090479415388, 'bleu': 0.20590181031343793, 'meteor': 0.3670692529604096, 'bertscore_f1': 0.8792363548519635}\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "test_metrics = evaluate_model_on_test(test_data)\n",
    "print(\"Evaluation Results on Test dataset:\")\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('t5_finetuned_5epochs/tokenizer_config.json',\n",
       " 't5_finetuned_5epochs/special_tokens_map.json',\n",
       " 't5_finetuned_5epochs/spiece.model',\n",
       " 't5_finetuned_5epochs/added_tokens.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained(\"t5_finetuned_5epochs\")\n",
    "tokenizer.save_pretrained(\"t5_finetuned_5epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text):\n",
    "    # Preprocess the input text\n",
    "    inputs = tokenizer(\n",
    "        f\"summarize: {text}\", \n",
    "        max_length=512, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Move tensors to the appropriate device\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    # Generate the summary (output IDs)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=150, num_beams=2, early_stopping=True)\n",
    "    \n",
    "    # Decode the generated summary IDs to text\n",
    "    summary = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: q4 partnership aims to leverage the expertise of both companies in artificial intelligence and machine learning.\n",
      "475 --> 112\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "Apple announced a new partnership with a leading tech firm, Google, to develop advanced AI solutions, in Quarter 4 of 2023. \n",
    "This collaboration aims to leverage the expertise of both companies in artificial intelligence and machine learning. \n",
    "The partnership is expected to accelerate innovation and improve product offerings in key markets. \n",
    "Both companies have expressed excitement about the potential of this joint venture to drive growth and create value for customers.\n",
    "\"\"\"\n",
    "summary = generate_summary(sample_text)\n",
    "print(\"Generated Summary:\", summary)\n",
    "print(len(sample_text), '-->', len(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: the partnership is expected to accelerate innovation and improve product offerings in key markets.\n",
      "335 --> 98\n"
     ]
    }
   ],
   "source": [
    "sample_text2 = \"\"\"\n",
    "Apple announced a new partnership with Google to develop advanced AI solutions, in Quarter 4 of 2023. \n",
    "Both companies have expressed excitement about the potential of this joint venture to drive growth and create value for customers.\n",
    "The partnership is expected to accelerate innovation and improve product offerings in key markets. \n",
    "\"\"\"\n",
    "summary2 = generate_summary(sample_text2)\n",
    "print(\"Generated Summary:\", summary2)\n",
    "print(len(sample_text2), '-->', len(summary2))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
